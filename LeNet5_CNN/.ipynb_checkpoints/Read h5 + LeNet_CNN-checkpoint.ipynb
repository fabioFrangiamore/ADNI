{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# librerie pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch.optim import Adam\n",
    "from torch.utils import data\n",
    "import torch.utils.data as utils\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "from scipy import stats\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comp = 'Poli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5Dataset(data.Dataset):\n",
    "    \"\"\"Represents an abstract HDF5 dataset.\n",
    "    \n",
    "    Input params:\n",
    "        file_path: Path to the folder containing the dataset (one or multiple HDF5 files).\n",
    "            the dataset is fits into memory. Otherwise, leave this at false and \n",
    "            the data will load lazily.\n",
    "        data_cache_size: Number of HDF5 files that can be cached in the cache (default=3).\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path,  data_cache_size):\n",
    "        super().__init__()\n",
    "        self.data_info = []\n",
    "        self.data_cache = {}\n",
    "        self.data_cache_size = data_cache_size\n",
    "\n",
    "        # Search for all h5 files\n",
    "        p = Path(file_path)\n",
    "        assert(p.is_dir())\n",
    "\n",
    "        files = sorted(p.glob('*.h5'))\n",
    "        if len(files) < 1:\n",
    "            raise RuntimeError('No hdf5 datasets found')\n",
    "\n",
    "        for h5dataset_fp in files:\n",
    "            self._add_data_infos(str(h5dataset_fp.resolve()))\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # get data\n",
    "        x = self.get_data(\"data\", index)\n",
    "\n",
    "        x = torch.from_numpy(x)\n",
    "\n",
    "        # get label\n",
    "        y = self.get_data(\"label\", index)\n",
    "        y = torch.from_numpy(y)\n",
    "        return (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.get_data_infos('data'))\n",
    "    \n",
    "    def _add_data_infos(self, file_path):\n",
    "        with h5py.File(file_path) as h5_file:\n",
    "            # Walk through all groups, extracting datasets\n",
    "            for gname, group in h5_file.items():\n",
    "                for dname, ds in group.items():\n",
    "                    # if data is not loaded its cache index is -1\n",
    "                    idx = -1\n",
    "                    \n",
    "                    # type is derived from the name of the dataset; we expect the dataset\n",
    "                    # name to have a name such as 'data' or 'label' to identify its type\n",
    "                    # we also store the shape of the data in case we need it\n",
    "                    self.data_info.append({'file_path': file_path, 'type': dname, 'shape': ds.value.shape, 'cache_idx': idx})\n",
    "\n",
    "    def _load_data(self, file_path):\n",
    "        \"\"\"Load data to the cache given the file\n",
    "        path and update the cache index in the\n",
    "        data_info structure.\n",
    "        \"\"\"\n",
    "        with h5py.File(file_path) as h5_file:\n",
    "            for gname, group in h5_file.items():\n",
    "                for dname, ds in group.items():\n",
    "                    # add data to the data cache and retrieve\n",
    "                    # the cache index\n",
    "                    idx = self._add_to_cache(ds.value, file_path)\n",
    "\n",
    "                    # find the beginning index of the hdf5 file we are looking for\n",
    "                    file_idx = next(i for i,v in enumerate(self.data_info) if v['file_path'] == file_path)\n",
    "\n",
    "                    # the data info should have the same index since we loaded it in the same way\n",
    "                    self.data_info[file_idx + idx]['cache_idx'] = idx\n",
    "\n",
    "        # remove an element from data cache if size was exceeded\n",
    "        if len(self.data_cache) > self.data_cache_size:\n",
    "            # remove one item from the cache at random\n",
    "            removal_keys = list(self.data_cache)\n",
    "            removal_keys.remove(file_path)\n",
    "            self.data_cache.pop(removal_keys[0])\n",
    "            # remove invalid cache_idx\n",
    "            self.data_info = [{'file_path': di['file_path'], 'type': di['type'], 'shape': di['shape'], 'cache_idx': -1} if di['file_path'] == removal_keys[0] else di for di in self.data_info]\n",
    "\n",
    "    def _add_to_cache(self, data, file_path):\n",
    "        \"\"\"Adds data to the cache and returns its index. There is one cache\n",
    "        list for every file_path, containing all datasets in that file.\n",
    "        \"\"\"\n",
    "        if file_path not in self.data_cache:\n",
    "            self.data_cache[file_path] = [data]\n",
    "        else:\n",
    "            self.data_cache[file_path].append(data)\n",
    "        return len(self.data_cache[file_path]) - 1\n",
    "\n",
    "    def get_data_infos(self, type):\n",
    "        \"\"\"Get data infos belonging to a certain type of data.\n",
    "        \"\"\"\n",
    "        data_info_type = [di for di in self.data_info if di['type'] == type]\n",
    "        return data_info_type\n",
    "\n",
    "    def get_data(self, type, i):\n",
    "        \"\"\"Call this function anytime you want to access a chunk of data from the\n",
    "            dataset. This will make sure that the data is loaded in case it is\n",
    "            not part of the data cache.\n",
    "        \"\"\"\n",
    "        fp = self.get_data_infos(type)[i]['file_path']\n",
    "        if fp not in self.data_cache:\n",
    "            self._load_data(fp)\n",
    "        \n",
    "        # get new cache_idx assigned by _load_data_info\n",
    "        cache_idx = self.get_data_infos(type)[i]['cache_idx']\n",
    "        return self.data_cache[fp][cache_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_h5(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, in_file, imgs_key='data', labels_key='label', transform = None):\n",
    "        super(dataset_h5, self).__init__()\n",
    "\n",
    "        self.in_file = in_file\n",
    "        self.imgs_key = imgs_key\n",
    "        self.labels_key = labels_key\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with h5py.File(self.in_file,'r') as db:\n",
    "            img_data = db.file[self.imgs_key][index]\n",
    "            label = db.file[self.labels_key][index]\n",
    "            if self.transform:\n",
    "                img_data = self.transform(img_data)\n",
    "        \n",
    "        return (img_data, label, index)\n",
    "\n",
    "    def __len__(self):\n",
    "        with h5py.File(self.in_file,'r') as db:\n",
    "            lens=len(db[self.labels_key])\n",
    "            \n",
    "        return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"label\": shape (618, 1), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    " with h5py.File('D:/ADNI/Dati/ADNI_T1/ADNI1_T1/ADNI_Registrate/H5Corrette/data_img_norm_bilanciate.h5','r') as db:\n",
    "        print(db['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Comp == 'FabioPC':\n",
    "    dataset = dataset_h5('/media/fabio/Disco locale/Fabio/Programmazione/Python/Poliambulanza/Alzheimer/Dati/ADNI/ADNI_Prova/H5Corrette/data_img.h5', transform)\n",
    "else:\n",
    "    dataset = dataset_h5('D:/ADNI/Dati/ADNI_T1/ADNI1_T1/ADNI_Registrate/H5Corrette/data_img_norm_intere.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181, 531, 364, 177, 593, 199, 421, 300, 408, 396, 286, 257, 259, 610, 42, 73, 537, 66, 11, 163, 443, 210, 332, 83, 278, 448, 79, 23, 287, 301, 280, 244, 290, 329, 137, 188, 165, 296, 9, 196, 231, 319, 604, 265, 84, 174, 547, 218, 316, 490, 390, 213, 153, 75, 92, 430, 68, 15, 192, 375, 88, 518, 117, 409, 434, 33, 0, 611, 553, 369, 355, 453, 551, 22, 472, 116, 89, 182, 495, 411, 18, 428, 535, 144, 302, 565, 557, 425, 272, 261, 362, 429, 167, 54, 441, 46, 93, 304, 108, 292, 195, 617, 513, 467, 370, 407, 7, 412, 423, 284, 275, 581, 69, 264, 432, 298, 249, 572, 274, 149, 124, 607, 530, 185, 333, 312, 477, 310, 609, 31, 586, 506, 568, 486, 141, 19, 172, 483, 482, 25, 446, 589, 605, 318, 245, 338, 154, 126, 367, 113, 173, 57, 344, 222, 17, 320, 255, 327, 591, 190, 341, 543, 291, 94, 180, 395, 354, 550, 334, 5, 45, 574, 416, 525, 16, 48, 597, 563, 3, 349, 555, 469, 388, 464, 394, 225, 26, 583, 263, 50, 229, 37, 157, 237, 592, 175, 519, 436, 194, 521, 383, 596, 527, 67, 414, 168, 500, 162, 309, 193, 152, 350, 360, 226, 356, 103, 419, 74, 451, 595, 119, 415, 438, 580, 207, 357, 526, 365, 433, 8, 404, 36, 139, 253, 303, 528, 59, 111, 501, 558, 380, 262, 515, 297, 150, 266, 534, 359, 546, 38, 514, 307, 198, 497, 381, 146, 503, 548, 147, 449, 348, 325, 420, 123, 536, 96, 143, 239, 403, 97, 371, 324, 279, 293, 587, 122, 183, 202, 323, 246, 447, 616, 512, 439, 545, 417, 125, 402, 456, 442, 223, 342, 53, 219, 129, 559, 422, 479, 561, 424, 100, 431, 386, 598, 294, 267, 494, 444, 112, 179, 450, 493, 481, 373, 314, 151, 171, 233, 306, 457, 164, 317, 136, 410, 197, 570, 258, 232, 115, 120, 463, 507, 376, 384, 224, 585, 488, 347, 127, 608, 445, 285, 569, 43, 107, 522, 556, 133, 440, 61, 44, 169, 65, 283, 85, 242, 186, 326, 159, 12, 35, 28, 170, 142, 236, 496, 516, 579, 221, 590, 95, 51, 240, 524, 351, 532, 178, 460, 542, 41, 571, 538, 206, 392, 282, 358, 397, 254, 517, 217, 4, 549, 98, 406, 502, 47, 32, 200, 134, 27, 540, 230, 489, 378, 288, 418, 391, 498, 138, 62, 471, 128, 601, 520, 64, 14, 156, 40, 492, 379, 187, 216, 52, 337, 295, 251, 461, 455, 615, 269, 201, 161, 401, 476, 105, 389, 1, 575, 80, 205, 34, 508, 427, 454, 366, 91, 339, 564, 345, 241, 13, 315, 387, 273, 166, 612, 484, 603, 504, 243, 566, 562, 189, 475, 510, 58, 474, 560, 252, 21, 313, 459, 160, 276, 191, 385, 413, 491, 343, 308, 130, 99, 372, 87, 458, 330, 214, 466, 121, 20, 71, 106, 270, 435, 102]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "print(train_indices)\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, \n",
    "                                           sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=3,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader_params = {'batch_size': 15, 'shuffle': False, 'num_workers': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_loader = data.DataLoader(dataset, **loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_loader.dataset.get_data(\"data\",0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_loader.dataset.get_data_infos(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_index, (img, labels, index) in enumerate(train_loader):\n",
    "    print(labels, index, img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.data_cache_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].imshow(img[0][80,:,:])\n",
    "axs[0, 1].imshow(img[1][80,:,:])\n",
    "axs[1, 0].imshow(img[2][80,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 5, kernel_size=(3, 3, 3))\n",
    "        self.conv2 = nn.Conv3d(5, 10, kernel_size=(3, 4, 4))\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(10, 15, kernel_size=(3, 5, 5))\n",
    "        self.conv4 = nn.Conv3d(15, 20, kernel_size=(4, 4, 4))\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear( 8 * 13 * 13 * 20, 50) # 38 * 61 * 61 * 20\n",
    "        self.fc2 = nn.Linear(50, 20)\n",
    "        self.fc3 = nn.Linear(20,3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool3d(F.relu(self.conv1(x)), (2, 2, 2))\n",
    "        x = F.max_pool3d(F.relu(self.conv2(x)), (2, 2, 2))\n",
    "        \n",
    "        x =F.max_pool3d(F.relu(self.conv3(x)), (2, 2, 2))\n",
    "        x = F.max_pool3d(F.relu(self.conv4(x)), (2, 2, 2))\n",
    "            \n",
    "            \n",
    "        \n",
    "        x = x.view(-1,13 * 13 * 20 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "weights = [1, 193/225, 194/199]\n",
    "class_weights = torch.FloatTensor(weights).cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 166, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([[0.3308, 0.3458, 0.3234]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([1])\n",
      "Train - Epoch 0, Batch: 0, Loss: 1.086190, Acc: 1\n",
      "torch.Size([1, 1, 166, 256, 256])\n",
      "\n",
      "tensor([[0.3295, 0.3473, 0.3232]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([0])\n",
      "Train - Epoch 0, Batch: 1, Loss: 1.102450, Acc: 0\n",
      "torch.Size([1, 1, 166, 256, 256])\n",
      "\n",
      "tensor([[0.3317, 0.3503, 0.3181]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([0])\n",
      "Train - Epoch 0, Batch: 2, Loss: 1.100348, Acc: 0\n",
      "torch.Size([1, 1, 166, 256, 256])\n",
      "\n",
      "tensor([[0.3344, 0.3501, 0.3155]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([2])\n",
      "Train - Epoch 0, Batch: 3, Loss: 1.116569, Acc: 0\n",
      "torch.Size([1, 1, 166, 256, 256])\n",
      "\n",
      "tensor([[0.3346, 0.3495, 0.3159]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([2])\n",
      "Train - Epoch 0, Batch: 4, Loss: 1.116124, Acc: 0\n",
      "torch.Size([1, 1, 166, 256, 256])\n",
      "\n",
      "tensor([[0.3368, 0.3460, 0.3173]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([1])\n",
      "Train - Epoch 0, Batch: 5, Loss: 1.086064, Acc: 1\n",
      "torch.Size([1, 1, 166, 256, 256])\n",
      "\n",
      "tensor([[0.3375, 0.3471, 0.3153]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([0])\n",
      "Train - Epoch 0, Batch: 6, Loss: 1.094511, Acc: 0\n",
      "torch.Size([1, 1, 166, 256, 256])\n",
      "\n",
      "tensor([[0.3356, 0.3473, 0.3171]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([2])\n",
      "Train - Epoch 0, Batch: 7, Loss: 1.114936, Acc: 0\n",
      "torch.Size([1, 1, 166, 256, 256])\n",
      "\n",
      "tensor([[0.3332, 0.3471, 0.3197]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([0])\n",
      "Train - Epoch 0, Batch: 8, Loss: 1.098844, Acc: 0\n",
      "torch.Size([1, 1, 166, 256, 256])\n",
      "\n",
      "tensor([[0.3405, 0.3450, 0.3145]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([0])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5d0476318546>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mtotal_acc\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train - Epoch %d, Batch: %d, Loss: %f, Acc: %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0,50):\n",
    "    \n",
    "    loss_list, batch_list = [], []\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for num, (img, label, index) in enumerate(train_loader):\n",
    "        \n",
    "        total = 0\n",
    "        correct=0\n",
    "        \n",
    "        inputs = img.unsqueeze(1)\n",
    "        print(inputs.shape)\n",
    "        label = np.argmax(label, axis=1)\n",
    "\n",
    "        output = model(inputs.float().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")))\n",
    "        loss = criterion(output, label.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), dtype= torch.long))\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        print()\n",
    "        print(output)\n",
    "        print(label)\n",
    "        \n",
    "        \n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))).sum().item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_list.append(loss.detach().cpu().item())\n",
    "        batch_list.append(i+1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_acc +=correct\n",
    "        total_loss += loss.detach().cpu().item()\n",
    "        if num%1==0:\n",
    "            print('Train - Epoch %d, Batch: %d, Loss: %f, Acc: %d' % (i, num, loss.detach().cpu().item(), correct))\n",
    "    print('ACC = ' +str(total_acc/617))\n",
    "    print('Loss = ' + str(total_loss/617))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd() + '/LeNet5_CNN.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct=0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        images, labels = data\n",
    "        print(labels)\n",
    "        outputs = net(images.unsqueeze(1).float().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        print(predicted.to(\"cpu\"))\n",
    "        correct += (predicted.to(\"cpu\") == np.argmax(labels, axis=1)).sum().item()\n",
    "        print(correct)\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
